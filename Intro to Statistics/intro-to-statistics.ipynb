{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Statistical Thinking \n\nüèÅ Welcome! This is Jupyter Notebook is all about statistics and how to think probablistically ","metadata":{"_uuid":"b39bf7de-b4d0-41a5-a8cb-de42da3a1088","_cell_guid":"9f4b6b9c-cf63-445e-9ae7-f19e0a0db13d","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## üìö Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n\nmy_colors = [\"#ce8f5a\", \"#efd199\", \"#80c8bc\", \"#5ec0ca\", \"#6287a2\"]\nsns.palplot(sns.color_palette(my_colors))\n\n# Set Style\n\nsns.set_style(\"white\")\nmpl.rcParams['xtick.labelsize'] = 16\nmpl.rcParams['ytick.labelsize'] = 16\nmpl.rcParams['axes.spines.left'] = False\nmpl.rcParams['axes.spines.right'] = False\nmpl.rcParams['axes.spines.top'] = False\n\nclass color:\n    BOLD = '\\033[1m' + '\\033[93m'\n    END = '\\033[0m'\n","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:00.743852Z","iopub.execute_input":"2021-10-12T16:19:00.744177Z","iopub.status.idle":"2021-10-12T16:19:02.082864Z","shell.execute_reply.started":"2021-10-12T16:19:00.744144Z","shell.execute_reply":"2021-10-12T16:19:02.082137Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":" # What is the purpose of this notebook ü§î \n \n <p>üü¢The main purpose is to discuss and refresh some information that is introduced on Stanford introduction to statistics course and to highlight basic but hidden important notes about the importants of statistics in the pocket tools for Data related fields and/or almost any job in real world </p>\n\n","metadata":{}},{"cell_type":"markdown","source":"## 1. what is data and why it is  important ?\n\nBefore discussing what is statistics, it's more important to introduce data, which I look at them as tools (statistics) and a material (data).We must know the types of data and how we want to shape it to deal with it with the right \"tools\".\n\n<b>Data</b> is defined as distinct pieces of information and it can come in many forms. From numbers in a spreadsheet, text to video and databases, to images and audio recordings, utilizing data in its different forms is the new way of the world.\n\nData is used to understand and improve nearly every facet of our lives. So, no matter what field you are in, you can utilize data to make better decisions and accomplish your goals.\n\nSo manily if any bussiness wants to evolve, expand, or fix issues in its system they will need to have a data about that thing they want to change. \n\n### <b>If you can‚Äôt measure it, you can‚Äôt improve it</b>\n","metadata":{}},{"cell_type":"markdown","source":" <img src=\"https://static.vecteezy.com/system/resources/previews/000/650/182/original/thinking-about-statistics-vector.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>","metadata":{}},{"cell_type":"markdown","source":"## 2. Elements of Structured Data\n\n","metadata":{}},{"cell_type":"markdown","source":"<b>Data</b> comes from many sources: sensor measurements, events, images, and videos. Much of this data is unstructured: images are a collection of pixels, with each pixel containing RGB color formation. Click Streams are sequences of  actions by user interacting with an app or web page. In fact, a major challenge of data science is to harness this torrent of raw data into actionable information. To apply statistical concepts, unstructured raw data must be processed and manipulated into a structured form. One of the commonest forms of structured data is a table with rows and columns.","metadata":{}},{"cell_type":"markdown","source":"<img src = \"https://lawtomated.com/wp-content/uploads/2019/04/structuredVsUnstructuredIgneos.png\" alt=\"Drawing\" style=\"width: 900px;\" />","metadata":{}},{"cell_type":"markdown","source":"## 3.Types of Structured data \n\n*There are two basic types of structured data: Numeric and Categorical. Numeric data comes in two forms: Continuous, such as wind speed or time duration, and discrete, such as the count of the occurrence of an event. Categorical data takes only a fixed set of values, such as a type of TV screen or a state name. Binary data is an important special case if categorical data that takes only one of two values , such as 0/1, yes/no. Another useful type of categorical data is Ordinal data in which the categories are ordered; an example of this is a numerical rating (1,2,3,4, or 5)*","metadata":{}},{"cell_type":"markdown","source":"<img src = \"http://intellspot.com/wp-content/uploads/2018/08/Types-of-Data-Infographic.png\" alt=\"Drawing\" style=\"width: 900px;\" />","metadata":{}},{"cell_type":"markdown","source":"## 4. Now why we need Statistics ?  üßê\n\n*We can Start by it's official definition to get a hint about it :*\n\n> Statistics is a method of interpreting, analysing and summarising the data. Hence, the types of statistics are categorised based on these features: Descriptive and inferential statistics. Based on the representation of data such as using pie charts, bar graphs, or tables, we analyse and interpret it.\n\n> In terms of mathematical analysis, the statistics include linear algebra, stochastic study, differential equation and measure-theoretic probability theory.\n","metadata":{}},{"cell_type":"markdown","source":"<img src = \"https://image.slideserve.com/274561/two-types-of-statistics-l.jpg\" alt=\"Drawing\" style=\"width: 900px;\" />","metadata":{}},{"cell_type":"markdown","source":"## 5. Descriptive Statistics \n\n**Summarize and organize characteristics of data set**\n\nThere are 3 main types of descriptive statistics:\n\n* The distribution concerns the frequency of each value.\n* The central tendency concerns the averages of the values.\n* The variability or dispersion concerns how spread out the values are.\n\n\n#### Frequency distribution \nA data set is made up of a distribution of values, or scores. In tables or graphs, you can summarize the frequency of every possible value of a variable in numbers or percentages. \n\n#### Estimates of Location \nVariables with measured or count data might have thousands of distinct values. Abasic step in exploring your data is getting a \"typical value\" for each feature (varaible):\nan estimate of where most of the data is located (i.e., its central tendency).\n\n**Mean**: The sum of all values divided by the number of values.  \n*Synonym*: \"Average\"\n\n**Median**: The value such that one-half of the data lies above and below.\n\n**Percentile**: The value such that *P* percent of the data lies below.  \n         *Synonym*: \"quantile\"\n\nAnd there are two important definations we will need to know about:\n\n**Robust**: Not Sensitive to extreme values.        \n \n**Outlier**: A data value that is very different from most of the data\n","metadata":{}},{"cell_type":"markdown","source":"![](https://media.giphy.com/media/9ADoZQgs0tyww/giphy.gif)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/inputdatasets/2008_all_states.csv\")\ndf.head()\n# this is a dataset that will be used to calarify more on the points that will be mentioned.\n# this dataset is about voters in different states in us for the 2008 election. ","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:02.085866Z","iopub.execute_input":"2021-10-12T16:19:02.086288Z","iopub.status.idle":"2021-10-12T16:19:02.148065Z","shell.execute_reply.started":"2021-10-12T16:19:02.086244Z","shell.execute_reply":"2021-10-12T16:19:02.147194Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# we can get to know about the number of voters in every state\n\ntot_mean = df.dem_share.mean()\n\ntot_median = df.dem_share.median()\n\nprint(tot_mean)\ntot_median\n\n## By knowing that (mean) is sensitive to outliers \n## And median is Robust. \n## We can conclude at first look that dem_share is some what normally distributed (number of outliers is not big )","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:02.149176Z","iopub.execute_input":"2021-10-12T16:19:02.150137Z","iopub.status.idle":"2021-10-12T16:19:02.163804Z","shell.execute_reply.started":"2021-10-12T16:19:02.150102Z","shell.execute_reply":"2021-10-12T16:19:02.162923Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"So by using easy descriptive stats mean,median we can have a first look for our dataset. \nBut this is not know for us to be sure about conclusions can be made only using one number stats like mean and median \nanother useful way is to visualize the distribution of the data to get to know the whole overview.\n\n**If you can appropriately display your data, you can already start to draw Conclusions from it.**","metadata":{}},{"cell_type":"markdown","source":"## 6. Exploratory Data Analysis (EDA)\n\nClassically,  statistics has been focused entirely on inference, drawing conclusions on large populations using small samples.  John W. Tukey wrote a seminal paper in 1962 called ‚Äú The Future of Data Analysis ‚Äú and proposed a new scientific discipline called data analysis which included statistical inference as one component but also considered engineering and computer science.\n\nThe field of exploratory data analysis was established with Tukey‚Äôs 1977 now-classic book ‚Äú Exploratory Data Analysis‚Äù.\n\n### Exploring the data\n The process of organizing, plotting, and       summarizing a dataset \n\n**We should explore the data first**\nthis involves taking data from tabular form and representing it graphically.\n\n*We are representing the same information, but it is in a more human-interpretable form*","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"## 6.1 The Importance of EDA: Anscombe's quartet\n\nIn 1973, statistian *\"Francis anscombe\"* published a paper that contained (4) fictitious x-y datasets, he uses these datasets to make an important point. That point becomes clear if we blindly go about doing parameter estimation on these datasets. First, look at the \"average\" x-values --> **They all have the same line**\n\nHow about the \"average\" y-values --> **Again, they are all the same.**\n\nwhat if we do linear regression on each of the data --> **They all have the same line.**\n\nand also by looking at the sum of the squares of the residuals --> They are basically the same.\n\n**Of course, anscombe constructed the datasets so that this would happen.The point he was making is very important.**\n\n**LOOK BEFORE YOU LEAP !  ==>   DO GRAPHICAL EDA FIRST.**\n\n*This is a powerful reminder to do some graphical exploratory data analysis before you start computing and making judgments about your data.*\n\n**Explore your data first.**","metadata":{}},{"cell_type":"markdown","source":"## 7. Graphical exploratory data analysis \n\nExploring the data is the process of organizing, plotting, and summarizing a dataset.\nWe should explore our data first this involves taking data from tabular form and representing it graphically.\nwe are representing the same information, but it is in more human-interpretable from. for example, we can take democratic share of votes and plot them as a histogram. Just by making one plot, we can could already draw a conclusion about the data.\n‚ÄúIt is best to use a graphical summary to communicate information, because people prefer to look at pictures rather than at numbers.‚Äù \n","metadata":{}},{"cell_type":"markdown","source":"<img src = \"https://media.giphy.com/media/TJP7EH5i1fB2rKeWbf/giphy.gif\" alt=\"Drawing\" style=\"width: 300px;\" />","metadata":{}},{"cell_type":"markdown","source":"### 7.1 Histogram","metadata":{}},{"cell_type":"code","source":"# for example, we take democratic share of vote\n# And plot them as a histogram ==>\n_ = sns.histplot(df['dem_share'], stat='percent', bins=20)\n_ = plt.xlabel(\"Percent of vote for obama\")\n_ = plt.ylabel(\"Number of countries\")\n","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:02.165991Z","iopub.execute_input":"2021-10-12T16:19:02.166248Z","iopub.status.idle":"2021-10-12T16:19:02.478989Z","shell.execute_reply.started":"2021-10-12T16:19:02.166220Z","shell.execute_reply":"2021-10-12T16:19:02.478209Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**A major drawback of using histograms**, is that the same dataset can look different depending on how the **bins** are chosen. And the choice of bins is in many ways arbitary.\n\n**Binning Bias**: the same data may be interpreted differently depending on choice of bins. \n\nAdditional problem with histograms is that we're not plotting all of the data, we are sweeping the data into bins, and losing their actual values.","metadata":{}},{"cell_type":"markdown","source":"## 7.2 Bee Swarm\n*The position along the y-axis is the 'quantitative information' the data is spread in x to make them visible.*\nNotably, we no longer have any binning bias and all data are displayed. \n**Note**: A requirment is that your data are in a \"well-organized\" pandas dataframe.\nwere each column is a feature & row is an observation. ","metadata":{}},{"cell_type":"code","source":"df_2 = df[df.state.isin(['PA','OH', 'FL'])]\ndf_2.head()\n## to get only the swing states ","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:02.480274Z","iopub.execute_input":"2021-10-12T16:19:02.480779Z","iopub.status.idle":"2021-10-12T16:19:02.498765Z","shell.execute_reply.started":"2021-10-12T16:19:02.480737Z","shell.execute_reply":"2021-10-12T16:19:02.497480Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"_ = sns.swarmplot(x='state', y='dem_share', data=df_2);\n_ = plt.xlabel(\"State\")\n_ = plt.ylabel(\"Percent of vote for obama\")","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:02.502268Z","iopub.execute_input":"2021-10-12T16:19:02.502536Z","iopub.status.idle":"2021-10-12T16:19:02.732284Z","shell.execute_reply.started":"2021-10-12T16:19:02.502502Z","shell.execute_reply":"2021-10-12T16:19:02.731631Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## 7.2 Empirical cumulative distribution function (ECDFs)\n\n**Why bee-swarm is not also the best option**\n> We saw the clarity of bee-swarm plots.However, there is a limit to their efficary. for example, imagine we wanted to plot the country-level voting data for all states. *The bee-swarm has a real problem, the edges have overlapping data points, which was nessary in order to fit all points onto the plot.*\n\n**ECDFs**: The x-axis is the quantity you are measuring. The y-axis is the fraction of data points that have a value smaller than the corresponding x-axis. \n*We can also easily plot multiple ECDFs*\n\n**Making an ECDF** ==> The x-axis us the sorted data.*We need to generate it using the numpy function \"sort\"*. np.sort(x) = X\n\n==> The y-axis is evenly spaced data points with a maximum of one.\n\n*Which we can generate using* => np.arange(1,len(x)+1) / len(x)  == Y\n\n**As its a repeatable process we can make a function for it **","metadata":{}},{"cell_type":"code","source":"def ecdf(data):\n    \"\"\"Compute ECDF for a 1-D array of measurments.\"\"\"\n    \n    # number of data points: n \n    n = len(data)\n    \n    # x-data for the ECDF: x\n    x = np.sort(data)\n    \n    # y-data for the ECDF: y\n    y = np.arange(1, len(x)+1) / n\n    \n    return x, y","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:02.733452Z","iopub.execute_input":"2021-10-12T16:19:02.733999Z","iopub.status.idle":"2021-10-12T16:19:02.741128Z","shell.execute_reply.started":"2021-10-12T16:19:02.733905Z","shell.execute_reply":"2021-10-12T16:19:02.740233Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"x, y = ecdf(df['dem_share'])\n\nplt.plot(x,y, marker='.', linestyle = 'none')\n\nplt.xlabel(\"Percent of vote for obama\")\n\nplt.ylabel(\"ECDF\")\n\n# keep data off plot edges\nplt.margins(0.02)","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:02.742963Z","iopub.execute_input":"2021-10-12T16:19:02.743594Z","iopub.status.idle":"2021-10-12T16:19:03.010036Z","shell.execute_reply.started":"2021-10-12T16:19:02.743545Z","shell.execute_reply":"2021-10-12T16:19:03.008635Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## 8. Quantitative Exploratory Data Analysis\n\n> We often would like to summarize data even more succinctly, say in one or two numbers. \n\nThese numerical summaries are **not by any stretch** a substitute for the graphical methods , they do take up less real state. ","metadata":{}},{"cell_type":"code","source":"z = df_2[df_2.state == 'PA']\nnp.mean(z.dem_share) # this computes the mean for a column.","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:03.011657Z","iopub.execute_input":"2021-10-12T16:19:03.011901Z","iopub.status.idle":"2021-10-12T16:19:03.019456Z","shell.execute_reply.started":"2021-10-12T16:19:03.011874Z","shell.execute_reply":"2021-10-12T16:19:03.018726Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"We might like a summary statistic that is immune to extreme data.\n**The median**: provides exactly that, the middle value of a dataset.","metadata":{}},{"cell_type":"code","source":"np.median(z.dem_share)","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:03.022033Z","iopub.execute_input":"2021-10-12T16:19:03.022405Z","iopub.status.idle":"2021-10-12T16:19:03.111163Z","shell.execute_reply.started":"2021-10-12T16:19:03.022375Z","shell.execute_reply":"2021-10-12T16:19:03.110502Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## 8.1 Percentiles, outliers, and boxplots\n*The median is the special name for the (50th percentile), that is 50% of the data are less than the median.*\n\nSimilarly, the(25th percentile) is the value of the data point that is greater than 25% of the sorted data, and so on for any other percentile we want.\n\n**Percentiles** are useful summary statistics, and can be computed --> np.percentile()\n*It takes a column data as a first argument and a list of the percentiles we want as a second argument (Percentiles NOT Fractions)*\n\n**Now, we've three summary statistics.**The whole point of summary statistics was to keep things concise, but we're starting to get a lot of numbers here.\n\n**Dealing with this issue is where \"Quantitative\" EDA meets \"Graphical\" EDA**\n\n**Box Plots** --> were invented to display some of the statistical or salient features of the dataset based on percentiles.\n\n==> The center of the box is the median (50th percentile), the edges of the box are the 25th & 75th percentiles. The total height of the box contains the middle 50% of the data, and is called (the interquartile) range, the whiskers extend a distance of (1.5) * IQR, or the extend of the data.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/max/8000/1*0MPDTLn8KoLApoFvI0P2vQ.png\" alt=\"Drawing\" style=\"width: 900px;\" />","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:03.112374Z","iopub.execute_input":"2021-10-12T16:19:03.112729Z","iopub.status.idle":"2021-10-12T16:19:03.134732Z","shell.execute_reply.started":"2021-10-12T16:19:03.112698Z","shell.execute_reply":"2021-10-12T16:19:03.133830Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"percentiles = np.array([2.5, 25, 50, 75, 97.5])\n                        \nvoters_percentiles = np.percentile(df.dem_share, percentiles)\nprint(voters_percentiles)","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:03.136102Z","iopub.execute_input":"2021-10-12T16:19:03.136612Z","iopub.status.idle":"2021-10-12T16:19:03.150538Z","shell.execute_reply.started":"2021-10-12T16:19:03.136555Z","shell.execute_reply":"2021-10-12T16:19:03.149171Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"_ = sns.boxplot(x='east_west', y='dem_share', data=df)\n\n_ = plt.xlabel(\"region\")\n_ = plt.ylabel(\"Percent of vote for Obama\")","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:03.152016Z","iopub.execute_input":"2021-10-12T16:19:03.152388Z","iopub.status.idle":"2021-10-12T16:19:03.374782Z","shell.execute_reply.started":"2021-10-12T16:19:03.152346Z","shell.execute_reply":"2021-10-12T16:19:03.373815Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## 8.2 Variance and standard deviation \n\n**What about the variability, or the spread of the data ?**\n\n**Variance** --> the mean (Average) squared distance of the data from their mean. To quantify this spread, informally, a measure of the spread of the data.\n\n$$ For each data point, we square the distance from the mean, and then take the average of all of these values.   \n\nthis is calculated with --> np.var() \n \n\n<b> Now, because the calculations of the varience invloves squared quantities, it does not have the same units of what we've measured. Therefore, we are interested in the square root of the varience.\n </b>\n \n <b>Standard Deviation</b> --> Is the square root of the varience. \n \n **A low standard deviation indicates that the values tend to be close to the mean of the set, while a high standard deviation indicates that the values are spread out over a wider range.**\n \n this is calculated with --> np.std() \n \n *Is a reasonable metric for the typical spread of the data.*","metadata":{}},{"cell_type":"code","source":"# Array of differences to mean: differences\ndifferences = df.dem_share - np.mean(df.dem_share)\n\n\n# Square the differences: diff_sq\ndiff_sq = differences ** 2\n\n# Compute the mean square difference: variance_explicit\nvariance_explicit = np.mean(diff_sq)\n\n# Compute the variance using NumPy: variance_np\n\nvariance_np = np.var(df.dem_share)\n# Print the results\nprint(variance_explicit , variance_np)","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:03.376241Z","iopub.execute_input":"2021-10-12T16:19:03.376479Z","iopub.status.idle":"2021-10-12T16:19:03.384743Z","shell.execute_reply.started":"2021-10-12T16:19:03.376452Z","shell.execute_reply":"2021-10-12T16:19:03.383603Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Compute the variance: variance\nvariance = np.var(df.dem_share)\n\n# Print the square root of the variance\nprint(np.sqrt(variance))\n\n# Print the standard deviation\nprint(np.std(df.dem_share))","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:03.386530Z","iopub.execute_input":"2021-10-12T16:19:03.386826Z","iopub.status.idle":"2021-10-12T16:19:03.400343Z","shell.execute_reply.started":"2021-10-12T16:19:03.386797Z","shell.execute_reply":"2021-10-12T16:19:03.399374Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## 8.3 Covarience and the pearson correlation coefficient\n\n*We would like to have a summary statistic to go along with the information we have just gleaned from the scatter plot below.*\n\n**Covarience** --> A measure of how two quantities vary together.  \n\n*np.cov()*\n\nto understand where it comes from, let's annotate the scatter plot with the means of the two quantities we're interested in.\n\nThe covarience is the mean of the product of these differences. \n\n**If (x) & (y) both tend to be above, or both below their respective means together.**\n\nas they are in this dataset, then the covariance is positive. \n\n*This means that they are positively correlated: when (x) is high so is (y)*\n\nConversely, if (x) is high while (y) is low, the coveriance is negative.\n\n**If we want to have a more generally applicable measure of how two variables depend on each other, we want it to be dimensionless, that is to not have any units.**\n\n**Pearson correlation coefficient** --> It is a comparison of the variability in the data due to covarience to the variablility inherent to each variable independently (their standard deviation ) * It ranges from (-1) to (1)*\n\nnp.corrcoef()\n\nAnother definition for PCC -->  is the covariance of the two variables divided by the product of their standard deviations.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://usersolutions.com/wp-content/uploads/2014/03/Covariance.png\" alt=\"Drawing\" style=\"width: 350px;\" />","metadata":{}},{"cell_type":"markdown","source":"![image.png](attachment:80af24e3-34da-43a4-b4ec-fe1515816123.png)","metadata":{},"attachments":{"80af24e3-34da-43a4-b4ec-fe1515816123.png":{"image/png":"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"}}},{"cell_type":"code","source":"_ = sns.scatterplot(x='total_votes', y='dem_share', data=df_2)\n_ = plt.xlabel(\"Percent of vote for Obama\")\n_ = plt.ylabel(\"Total votes (thousands)\")","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:03.402332Z","iopub.execute_input":"2021-10-12T16:19:03.402870Z","iopub.status.idle":"2021-10-12T16:19:03.707215Z","shell.execute_reply.started":"2021-10-12T16:19:03.402834Z","shell.execute_reply":"2021-10-12T16:19:03.706251Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def pearson_r(x, y):\n    \"\"\"Compute Pearson correlation coefficient between two arrays.\"\"\"\n    # Compute correlation matrix: corr_mat\n    corr_mat = np.corrcoef(x,y)\n\n    # Return entry [0,1]\n    return corr_mat[0,1]","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:03.708590Z","iopub.execute_input":"2021-10-12T16:19:03.708845Z","iopub.status.idle":"2021-10-12T16:19:03.713783Z","shell.execute_reply.started":"2021-10-12T16:19:03.708815Z","shell.execute_reply":"2021-10-12T16:19:03.712545Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df_2.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:03.715117Z","iopub.execute_input":"2021-10-12T16:19:03.715382Z","iopub.status.idle":"2021-10-12T16:19:03.743137Z","shell.execute_reply.started":"2021-10-12T16:19:03.715354Z","shell.execute_reply":"2021-10-12T16:19:03.741992Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"_ = sns.scatterplot(x='dem_votes', y='rep_votes', data=df_2)\n_ = plt.xlabel(\"Percent of vote for Obama\")\n_ = plt.ylabel(\"Total votes (thousands)\")","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:03.744559Z","iopub.execute_input":"2021-10-12T16:19:03.744886Z","iopub.status.idle":"2021-10-12T16:19:04.024132Z","shell.execute_reply.started":"2021-10-12T16:19:03.744843Z","shell.execute_reply":"2021-10-12T16:19:04.023140Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Compute Pearson correlation coefficient \nr = pearson_r(df_2.dem_votes , df_2.rep_votes)\n\n# Print the result\nprint(r)","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:04.025591Z","iopub.execute_input":"2021-10-12T16:19:04.025941Z","iopub.status.idle":"2021-10-12T16:19:04.032519Z","shell.execute_reply.started":"2021-10-12T16:19:04.025897Z","shell.execute_reply":"2021-10-12T16:19:04.031833Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## 9. Probablistic logic and statistical inference.\n*Probablistic reasoning allows us to describe uncertainty.*\n\nfor example, though you can't tell me exactly what the mean of the next 50 petal lengths you measure will be, you could say \"That is more probably to be close to what you got in the first 50 measurments that it is to be much greater.\"\n\n<b>That is what probablistic thinking is all about --> given a set of data, you describe probablistically what you might expect if those data were acquired again and again <b/>\n    \n<b>This is the heart of statistical inference</b>\n\n<b>*It is the process by which we go from measured data to probablistic conclusion about what we might expect if we collected the same data again.* </b>\n    \n \"Your Data Speak In The Language Of Probablility\"","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://media.giphy.com/media/TWFrH2pBDUCnzwKsQz/giphy.gif\" alt=\"Drawing\" style=\"width: 350px;\" />","metadata":{}},{"cell_type":"markdown","source":"## 9.1 Discrete variables (Random number generator & hacker statistics)\n\nIn practice, we're going to think probablistically using hacker stats.\n\n**Hacker Statistics --> Uses simulated repeated measurments to compute probabilities**\n\n*np.random()* module, a suite of functions based on random number generation.\n\n*np.random.random()* --> Draw a number between 0 and 1. \n\n**Bernolli trail --> *An experiment that has two options, 'Success'(True) and 'Fail'(False)***\n\n*np.random.seed()* --> integer fed into random number generating algorithm. Manually seed random number generator if you need reproducibility.\n\n**Hacker stats probabilities --> Determine how to simulate data, simulate it many many times. Probability is approximately fraction of trails with the outcome of interest.** ","metadata":{}},{"cell_type":"code","source":"## E.x : Coin flips  \nnp.random.random(size=4)","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:04.033397Z","iopub.execute_input":"2021-10-12T16:19:04.033969Z","iopub.status.idle":"2021-10-12T16:19:04.048348Z","shell.execute_reply.started":"2021-10-12T16:19:04.033940Z","shell.execute_reply":"2021-10-12T16:19:04.047572Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"## We want to know the probability of getting (4) heads if we were to repeat the four flips over and over again. (Using for loop)\n\nn_all_heads = 0 # we first initialize the counter to zero \n\nfor _ in range(10000):\n    \n    heads = np.random.random(size=4) < 0.5\n    n_heads = np.sum(heads)\n    \n    if n_heads == 4:\n        n_all_heads += 1\n        \nn_all_heads / 10000  ","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:04.049304Z","iopub.execute_input":"2021-10-12T16:19:04.050121Z","iopub.status.idle":"2021-10-12T16:19:04.232873Z","shell.execute_reply.started":"2021-10-12T16:19:04.050061Z","shell.execute_reply":"2021-10-12T16:19:04.231936Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## 9.2 Probability distributions and stories : The binomial distribution\n\n**Probability mass function (PMF) --> *The set of probabilities of descrete outcomes.***\n\nTo understand how this works, \n\n(e.x)- a person rolling a die once.\n\nThe outcomes are discrete because only certain values may be attained. Each result has the same or uniform probability (1/6).\n\nThe PMF associated with this story is called --> Discrete uniform PMF.\n\nNow the PMF is a property of a discrete probability distribution.\n\n**Probability distribution --> A mathematical description of outcomes.**\n\n**Discrete uniform distribution: The story --> \"the outcome of rolling a single fair die is:\"**\n\n*outcome* == DISCRETE #####################  *rolling* == UNIFORMALY DISTRIBUTED\n\n**Binomial  distribution: The story --> \"the number (r) of successes in (n) bernolli trails with probability (p) of success.\"**\n\nwith probability (p) of success ==> *np.random.binomial(trails, probab. of success, size = 10)*\n\nthe \"ECDF\" is just informative and easier to plot. \n\n*(Size) key-word argument, which tells the fraction how many random numbers to sample out of the binomial distribution.*","metadata":{}},{"cell_type":"code","source":"def perform_bernolli_trails(n, p):\n    \"\"\"perform (n) Bernolli trails with success probability (p) \n       and return number of successes.\"\"\"\n    \n    # Initialize number of successes: n_success\n    n_success = 0 \n    \n    # Perform trails\n    for i in range(n):\n        # choose random number between zero & one: random_number\n        random_number = np.random.random()\n        \n        # If less than (p), it is a success so add one to n_success\n        if random_number < p:\n            n_success += 1\n            \n    return n_success","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:04.235398Z","iopub.execute_input":"2021-10-12T16:19:04.235756Z","iopub.status.idle":"2021-10-12T16:19:04.242222Z","shell.execute_reply.started":"2021-10-12T16:19:04.235705Z","shell.execute_reply":"2021-10-12T16:19:04.241241Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Seed random number generator \nnp.random.seed(42)\n\n#Initialize the number of defaults: n_defaults \nn_defaults = np.empty(1000)\n\n# compute the number of defaults \nfor i in range(1000):\n    n_defaults[i] = perform_bernolli_trails(100, 0.05)\n    \n# plot the histogram with the default number of bins\n\n_ = plt.hist(n_defaults)\n_ = plt.xlabel('number of defaults out of 100 loans')\n_ = plt.ylabel('probability')\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:04.243726Z","iopub.execute_input":"2021-10-12T16:19:04.244035Z","iopub.status.idle":"2021-10-12T16:19:04.821470Z","shell.execute_reply.started":"2021-10-12T16:19:04.243996Z","shell.execute_reply":"2021-10-12T16:19:04.820494Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Compute ECDF: x, y\nx, y  = ecdf(n_defaults)\n\n# Plot the ECDF with labeled axes\n_ = plt.plot(x, y, marker = '.', linestyle = 'none')\n_ = plt.xlabel('number of defaults out of 100')\n_ = plt.ylabel('CDF')\n\nplt.show()\n# Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money\nn_lose_money = np.sum(n_defaults >= 10)\n\n# Compute and print probability of losing money\nprint('Probability of losing money =', n_lose_money / len(n_defaults))\n","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:04.822940Z","iopub.execute_input":"2021-10-12T16:19:04.823466Z","iopub.status.idle":"2021-10-12T16:19:05.056905Z","shell.execute_reply.started":"2021-10-12T16:19:04.823428Z","shell.execute_reply":"2021-10-12T16:19:05.055803Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## 9.2.1 Poisson processes and the Poisson distribution \n\n**Poisson process --> *The timing of the next event is completely independent of when the previous event happened.***\n\n*Example:* natural birth in a given hospital; hit a website in a given hour.\n\n--> The number of arrivals of \"Poisson process\" in a given amount of time is \"Poisson distribution\"\n\n**Poisson distribution --> has one parameter, the average number of arrivals in a given length of time.**\n\n*Example:* the number (r) of hits on a website in one hour with average hit rate of (6) hits per hour is poisson distributed.\n\n*The poisson distribution is a limit of the binomial distribution for low probability of success and large number of trials. That is,for rare events*\n\n**Note(1): The (std) of the binomial distribution gets closer and closer to that of the poisson distribution as the probability (p) gets lower and lower.**\n\n**Note(2): When we've rare events that is ( low(p) & high(n) ), the binomial distribution is poisson distribution.** \n\nnp.random.poisson(mean, size = 10000) === samples ","metadata":{}},{"cell_type":"code","source":"# Draw 10,000 samples out of Poisson distribution: samples_poisson\nsamples_poisson = np.random.poisson(10, size = 10000)\n\nprint('Poisson:     ', np.mean(samples_poisson),\n                       np.std(samples_poisson))\n\n# specify values of n and p to consider for binomial: n, p\n\nn = [20,100, 1000]\np = [0.5, 0.1, 0.01]\n\n# Draw 10,0000 samples for each n,p pair: samples_binomial\nfor i in range(3):\n    samples_binomial = np.random.binomial(n[i], p[i], size = 10000)\n    \n    print('n =', n[i], 'Binomial:', np.mean(samples_binomial),\n                                    np.std(samples_binomial))","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:05.060613Z","iopub.execute_input":"2021-10-12T16:19:05.060884Z","iopub.status.idle":"2021-10-12T16:19:05.075988Z","shell.execute_reply.started":"2021-10-12T16:19:05.060855Z","shell.execute_reply":"2021-10-12T16:19:05.074925Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## 10. Continuous Variables \n**It's time to move onto continuous variables, such as those that can take on any fractional value. Many of the principles are the same, but there are some subtleties.**\n\n**Probability density functions (PDF) --> continuous analog of (PMF), it describes the chance of observing a value of continuous variable. It's also mathematical description of the relative likelihood of observing a value of a continuous variable.**\n\n*Example: * Michelson's speed of light experiment, 100 measurements of the speed of light in air.Each measurement has some error in it; conditions, such as temperature, humidity, alignment of his optics, change from measurement to measurement.\n\n*So the probability of observing a single value of the speed of light does not make sense, because there is an infinity of numbers, say  between 299.6 and 300.0 megameters per second.Instead, **areas under the PDF give probabilities**. *\n\n*So, the probability of measuring that the speed of light is greater than 300,000 km/s is an area under the normal curve. (3%) chance.*\n\nTo do this calculation, we were really just looking at the cumulative distribution function (CDF), of the Normal distribution.","metadata":{}},{"cell_type":"code","source":"## sol == Speed Of Light \nsol = pd.read_csv(\"../input/inputdatasets/michelson_speed_of_light.csv\")\nsol.head()\n### We are intrested in the \"velocity of light in air (km/s)\" column ","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:05.077495Z","iopub.execute_input":"2021-10-12T16:19:05.077831Z","iopub.status.idle":"2021-10-12T16:19:05.111023Z","shell.execute_reply.started":"2021-10-12T16:19:05.077790Z","shell.execute_reply":"2021-10-12T16:19:05.110230Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"## PDF\n_ = sns.distplot(sol['velocity of light in air (km/s)'])\n_ = plt.xlabel('Speed of light (km/s)')\n_ = plt.ylabel('PDF')","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:05.115240Z","iopub.execute_input":"2021-10-12T16:19:05.115486Z","iopub.status.idle":"2021-10-12T16:19:05.409013Z","shell.execute_reply.started":"2021-10-12T16:19:05.115456Z","shell.execute_reply":"2021-10-12T16:19:05.407960Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"## CDF\nx, y = ecdf(sol['velocity of light in air (km/s)'])\n\n_ = plt.plot(x, y, marker = '.', linestyle = 'none')\n_ = plt.xlabel('Speed of light (km/s)')\n_ = plt.ylabel('CDF')\n","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:05.410565Z","iopub.execute_input":"2021-10-12T16:19:05.410897Z","iopub.status.idle":"2021-10-12T16:19:05.682195Z","shell.execute_reply.started":"2021-10-12T16:19:05.410846Z","shell.execute_reply":"2021-10-12T16:19:05.681267Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## 10.1 Introduction to the Normal distribution \n\n**Normal Distribution --> describes a continuous variable whose PDF has a single symmetric peak.**\n\n***The normal distribution is parametrized by two parameters.***\n\nThe (Mean) determines where the center of the peak is.\n\nThw (standard deviation) is a measure of how wide the peak is, or how spread out the data are.\n\n**Comparing the histogram to the (PDF) suffers from binning bias, so it is better to compare the (ECDF) of the data to the theoritical (CDF) of the normal distribution.**\n\n**the mean & std computed from the data are a good estimate, so we'll compute them and pass them into the function --> np.random.normal(mean, std, size = 10000)**\n\n*To compute \"Theoritical CDF\" --> np.random.normal(mean, std, size = 10000)*\n\nTo draw samples and then we can compute the CDF.\n\nFinally, we plot both the theoretical and empirical CDFs on the same plot.\n\n**NOTE: *that the mean & std are the names of the parameters of the normal distribution. Do not confuse these with the mean & std that we computed directly from the data when doing EDA.***","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://cdn-images-1.medium.com/max/2600/1*IdGgdrY_n_9_YfkaCh-dag.png\" alt=\"Drawing\" style=\"width: 700px;\"/>","metadata":{}},{"cell_type":"code","source":"## Checking Normality of Michelson data\n\n# the mean & std computed from the data are good estiates, so we'll compute them and pass them into the function.\nmean = np.mean(sol['velocity of light in air (km/s)'])\nstd = np.std(sol['velocity of light in air (km/s)'])\n# normally distributed theoretical samples \nsamples = np.random.normal(mean, std, size = 10000)\n\nx,y = ecdf(sol['velocity of light in air (km/s)'])\n\nx_theor, y_theor = ecdf(samples)\n\nsns.set()\n\n_ = plt.plot(x_theor, y_theor)\n_ = plt.plot(x, y, marker='.', linestyle = 'none')\n\n_ = plt.xlabel(\"Speed of light (km/s)\")\n_ = plt.ylabel(\"CDF\")\n","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:19:23.407571Z","iopub.execute_input":"2021-10-12T16:19:23.408518Z","iopub.status.idle":"2021-10-12T16:19:23.674458Z","shell.execute_reply.started":"2021-10-12T16:19:23.408472Z","shell.execute_reply":"2021-10-12T16:19:23.673537Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"with the absence of binning bias, it is much clearer that the \"Michelson\" data are Normally distributed.","metadata":{}},{"cell_type":"code","source":"# Draw 100000 samples from Normal distribution with stds of interest: samples_std1, samples_std3, samples_std10\nsamples_std1 = np.random.normal(20,1,size=100000)\nsamples_std3 = np.random.normal(20,3,size=100000)\nsamples_std10 = np.random.normal(20,10,size=100000)\n\n\n\n# Make histograms\nsns.distplot(samples_std1)\nsns.distplot(samples_std3)\nsns.distplot(samples_std10)\n\n\n# Make a legend, set limits and show plot\n_ = plt.legend(('std = 1', 'std = 3', 'std = 10'))\nplt.ylim(-0.01, 0.42)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-10-12T16:25:24.519898Z","iopub.execute_input":"2021-10-12T16:25:24.520747Z","iopub.status.idle":"2021-10-12T16:25:27.152729Z","shell.execute_reply.started":"2021-10-12T16:25:24.520705Z","shell.execute_reply":"2021-10-12T16:25:27.151773Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}