{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Statistical Thinking \n\nüèÅ Welcome! This is Jupyter Notebook is all about statistics and how to think probablistically ","metadata":{"_uuid":"b39bf7de-b4d0-41a5-a8cb-de42da3a1088","_cell_guid":"9f4b6b9c-cf63-445e-9ae7-f19e0a0db13d","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## üìö Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n\nmy_colors = [\"#ce8f5a\", \"#efd199\", \"#80c8bc\", \"#5ec0ca\", \"#6287a2\"]\nsns.palplot(sns.color_palette(my_colors))\n\n# Set Style\n\nsns.set_style(\"white\")\nmpl.rcParams['xtick.labelsize'] = 16\nmpl.rcParams['ytick.labelsize'] = 16\nmpl.rcParams['axes.spines.left'] = False\nmpl.rcParams['axes.spines.right'] = False\nmpl.rcParams['axes.spines.top'] = False\n\nclass color:\n    BOLD = '\\033[1m' + '\\033[93m'\n    END = '\\033[0m'\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T20:05:25.66147Z","iopub.execute_input":"2021-10-10T20:05:25.661755Z","iopub.status.idle":"2021-10-10T20:05:26.780152Z","shell.execute_reply.started":"2021-10-10T20:05:25.661724Z","shell.execute_reply":"2021-10-10T20:05:26.779219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # What is the purpose of this notebook ü§î \n \n <p>üü¢The main purpose is to discuss and refresh some information that is introduced on Stanford introduction to statistics course and to highlight basic but hidden important notes about the importants of statistics in the pocket tools for Data related fields and/or almost any job in real world </p>\n\n","metadata":{}},{"cell_type":"markdown","source":"## 1. what is data and why it is  important ?\n\nBefore discussing what is statistics, it's more important to introduce data, which I look at them as tools (statistics) and a material (data).We must know the types of data and how we want to shape it to deal with it with the right \"tools\".\n\n<b>Data</b> is defined as distinct pieces of information and it can come in many forms. From numbers in a spreadsheet, text to video and databases, to images and audio recordings, utilizing data in its different forms is the new way of the world.\n\nData is used to understand and improve nearly every facet of our lives. So, no matter what field you are in, you can utilize data to make better decisions and accomplish your goals.\n\nSo manily if any bussiness wants to evolve, expand, or fix issues in its system they will need to have a data about that thing they want to change. \n\n### <b>If you can‚Äôt measure it, you can‚Äôt improve it</b>\n","metadata":{}},{"cell_type":"markdown","source":" <img src=\"https://static.vecteezy.com/system/resources/previews/000/650/182/original/thinking-about-statistics-vector.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>","metadata":{}},{"cell_type":"markdown","source":"## 2. Elements of Structured Data\n\n","metadata":{}},{"cell_type":"markdown","source":"<b>Data</b> comes from many sources: sensor measurements, events, images, and videos. Much of this data is unstructured: images are a collection of pixels, with each pixel containing RGB color formation. Click Streams are sequences of  actions by user interacting with an app or web page. In fact, a major challenge of data science is to harness this torrent of raw data into actionable information. To apply statistical concepts, unstructured raw data must be processed and manipulated into a structured form. One of the commonest forms of structured data is a table with rows and columns.","metadata":{}},{"cell_type":"markdown","source":"<img src = \"https://lawtomated.com/wp-content/uploads/2019/04/structuredVsUnstructuredIgneos.png\" alt=\"Drawing\" style=\"width: 900px;\" />","metadata":{}},{"cell_type":"markdown","source":"## 3.Types of Structured data \n\n*There are two basic types of structured data: Numeric and Categorical. Numeric data comes in two forms: Continuous, such as wind speed or time duration, and discrete, such as the count of the occurrence of an event. Categorical data takes only a fixed set of values, such as a type of TV screen or a state name. Binary data is an important special case if categorical data that takes only one of two values , such as 0/1, yes/no. Another useful type of categorical data is Ordinal data in which the categories are ordered; an example of this is a numerical rating (1,2,3,4, or 5)*","metadata":{}},{"cell_type":"markdown","source":"<img src = \"http://intellspot.com/wp-content/uploads/2018/08/Types-of-Data-Infographic.png\" alt=\"Drawing\" style=\"width: 900px;\" />","metadata":{}},{"cell_type":"markdown","source":"## 4. Now why we need Statistics ?  üßê\n\n*We can Start by it's official definition to get a hint about it :*\n\n> Statistics is a method of interpreting, analysing and summarising the data. Hence, the types of statistics are categorised based on these features: Descriptive and inferential statistics. Based on the representation of data such as using pie charts, bar graphs, or tables, we analyse and interpret it.\n\n> In terms of mathematical analysis, the statistics include linear algebra, stochastic study, differential equation and measure-theoretic probability theory.\n","metadata":{}},{"cell_type":"markdown","source":"<img src = \"https://image.slideserve.com/274561/two-types-of-statistics-l.jpg\" alt=\"Drawing\" style=\"width: 900px;\" />","metadata":{}},{"cell_type":"markdown","source":"## 5. Descriptive Statistics \n\n**Summarize and organize characteristics of data set**\n\nThere are 3 main types of descriptive statistics:\n\n* The distribution concerns the frequency of each value.\n* The central tendency concerns the averages of the values.\n* The variability or dispersion concerns how spread out the values are.\n\n\n#### Frequency distribution \nA data set is made up of a distribution of values, or scores. In tables or graphs, you can summarize the frequency of every possible value of a variable in numbers or percentages. \n\n#### Estimates of Location \nVariables with measured or count data might have thousands of distinct values. Abasic step in exploring your data is getting a \"typical value\" for each feature (varaible):\nan estimate of where most of the data is located (i.e., its central tendency).\n\n**Mean**: The sum of all values divided by the number of values.  \n*Synonym*: \"Average\"\n\n**Median**: The value such that one-half of the data lies above and below.\n\n**Percentile**: The value such that *P* percent of the data lies below.  \n         *Synonym*: \"quantile\"\n\nAnd there are two important definations we will need to know about:\n\n**Robust**: Not Sensitive to extreme values.        \n \n**Outlier**: A data value that is very different from most of the data\n","metadata":{}},{"cell_type":"markdown","source":"![](https://media.giphy.com/media/9ADoZQgs0tyww/giphy.gif)","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/datauploaded/2008_all_states.csv\")\ndf.head()\n# this is a dataset that will be used to calarify more on the points that will be mentioned.\n# this dataset is about voters in different states in us for the 2008 election. ","metadata":{"execution":{"iopub.status.busy":"2021-10-10T20:05:26.782048Z","iopub.execute_input":"2021-10-10T20:05:26.782291Z","iopub.status.idle":"2021-10-10T20:05:26.833305Z","shell.execute_reply.started":"2021-10-10T20:05:26.782264Z","shell.execute_reply":"2021-10-10T20:05:26.832476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we can get to know about the number of voters in every state\n\ntot_mean = df.dem_share.mean()\n\ntot_median = df.dem_share.median()\n\nprint(tot_mean)\ntot_median\n\n## By knowing that (mean) is sensitive to outliers \n## And median is Robust. \n## We can conclude at first look that dem_share is some what normally distributed (number of outliers is not big )","metadata":{"execution":{"iopub.status.busy":"2021-10-10T20:05:26.834631Z","iopub.execute_input":"2021-10-10T20:05:26.834873Z","iopub.status.idle":"2021-10-10T20:05:26.84758Z","shell.execute_reply.started":"2021-10-10T20:05:26.834841Z","shell.execute_reply":"2021-10-10T20:05:26.846616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So by using easy descriptive stats mean,median we can have a first look for our dataset. \nBut this is not know for us to be sure about conclusions can be made only using one number stats like mean and median \nanother useful way is to visualize the distribution of the data to get to know the whole overview.\n\n**If you can appropriately display your data, you can already start to draw Conclusions from it.**","metadata":{}},{"cell_type":"markdown","source":"## 6. Exploratory Data Analysis (EDA)\n\nClassically,  statistics has been focused entirely on inference, drawing conclusions on large populations using small samples.  John W. Tukey wrote a seminal paper in 1962 called ‚Äú The Future of Data Analysis ‚Äú and proposed a new scientific discipline called data analysis which included statistical inference as one component but also considered engineering and computer science.\n\nThe field of exploratory data analysis was established with Tukey‚Äôs 1977 now-classic book ‚Äú Exploratory Data Analysis‚Äù.\n\n### Exploring the data\n The process of organizing, plotting, and       summarizing a dataset \n\n**We should explore the data first**\nthis involves taking data from tabular form and representing it graphically.\n\n*We are representing the same information, but it is in a more human-interpretable form*","metadata":{}},{"cell_type":"markdown","source":"## 7. Graphical exploratory data analysis \n\nExploring the data is the process of organizing, plotting, and summarizing a dataset.\nWe should explore our data first this involves taking data from tabular form and representing it graphically.\nwe are representing the same information, but it is in more human-interpretable from. for example, we can take democratic share of votes and plot them as a histogram. Just by making one plot, we can could already draw a conclusion about the data.\n‚ÄúIt is best to use a graphical summary to communicate information, because people prefer to look at pictures rather than at numbers.‚Äù \n","metadata":{}},{"cell_type":"markdown","source":"<img src = \"https://media.giphy.com/media/TJP7EH5i1fB2rKeWbf/giphy.gif\" alt=\"Drawing\" style=\"width: 300px;\" />","metadata":{}},{"cell_type":"markdown","source":"### 7.1 Histogram","metadata":{}},{"cell_type":"code","source":"# for example, we take democratic share of vote\n# And plot them as a histogram ==>\n_ = sns.histplot(df['dem_share'], stat='percent', bins=20)\n_ = plt.xlabel(\"Percent of vote for obama\")\n_ = plt.ylabel(\"Number of countries\")\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T20:05:26.849808Z","iopub.execute_input":"2021-10-10T20:05:26.850045Z","iopub.status.idle":"2021-10-10T20:05:27.149739Z","shell.execute_reply.started":"2021-10-10T20:05:26.850018Z","shell.execute_reply":"2021-10-10T20:05:27.148927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**A major drawback of using histograms**, is that the same dataset can look different depending on how the **bins** are chosen. And the choice of bins is in many ways arbitary.\n\n**Binning Bias**: the same data may be interpreted differently depending on choice of bins. \n\nAdditional problem with histograms is that we're not plotting all of the data, we are sweeping the data into bins, and losing their actual values.","metadata":{}},{"cell_type":"markdown","source":"## 7.2 Bee Swarm\n*The position along the y-axis is the 'quantitative information' the data is spread in x to make them visible.*\nNotably, we no longer have any binning bias and all data are displayed. \n**Note**: A requirment is that your data are in a \"well-organized\" pandas dataframe.\nwere each column is a feature & row is an observation. ","metadata":{}},{"cell_type":"code","source":"df_2 = df[df.state.isin(['PA','OH', 'FL'])]\ndf_2.head()\n## to get only the swing states ","metadata":{"execution":{"iopub.status.busy":"2021-10-10T20:05:27.150806Z","iopub.execute_input":"2021-10-10T20:05:27.151169Z","iopub.status.idle":"2021-10-10T20:05:27.171894Z","shell.execute_reply.started":"2021-10-10T20:05:27.15114Z","shell.execute_reply":"2021-10-10T20:05:27.171113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = sns.swarmplot(x='state', y='dem_share', data=df_2);\n_ = plt.xlabel(\"State\")\n_ = plt.ylabel(\"Percent of vote for obama\")","metadata":{"execution":{"iopub.status.busy":"2021-10-10T20:05:27.173067Z","iopub.execute_input":"2021-10-10T20:05:27.173372Z","iopub.status.idle":"2021-10-10T20:05:27.390132Z","shell.execute_reply.started":"2021-10-10T20:05:27.17333Z","shell.execute_reply":"2021-10-10T20:05:27.389466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.2 Empirical cumulative distribution function (ECDFs)\n\n**Why bee-swarm is not also the best option**\n> We saw the clarity of bee-swarm plots.However, there is a limit to their efficary. for example, imagine we wanted to plot the country-level voting data for all states. *The bee-swarm has a real problem, the edges have overlapping data points, which was nessary in order to fit all points onto the plot.*\n\n**ECDFs**: The x-axis is the quantity you are measuring. The y-axis is the fraction of data points that have a value smaller than the corresponding x-axis. \n*We can also easily plot multiple ECDFs*\n\n**Making an ECDF** ==> The x-axis us the sorted data.*We need to generate it using the numpy function \"sort\"*. np.sort(x) = X\n\n==> The y-axis is evenly spaced data points with a maximum of one.\n\n*Which we can generate using* => np.arange(1,len(x)+1) / len(x)  == Y\n\n**As its a repeatable process we can make a function for it **","metadata":{}},{"cell_type":"code","source":"def ecdf(data):\n    \"\"\"Compute ECDF for a 1-D array of measurments.\"\"\"\n    \n    # number of data points: n \n    n = len(data)\n    \n    # x-data for the ECDF: x\n    x = np.sort(data)\n    \n    # y-data for the ECDF: y\n    y = np.arange(1, len(x)+1) / n\n    \n    return x, y","metadata":{"execution":{"iopub.status.busy":"2021-10-10T20:05:27.391284Z","iopub.execute_input":"2021-10-10T20:05:27.392267Z","iopub.status.idle":"2021-10-10T20:05:27.399832Z","shell.execute_reply.started":"2021-10-10T20:05:27.392214Z","shell.execute_reply":"2021-10-10T20:05:27.398747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = ecdf(df['dem_share'])\n\nplt.plot(x,y, marker='.', linestyle = 'none')\n\nplt.xlabel(\"Percent of vote for obama\")\n\nplt.ylabel(\"ECDF\")\n\n# keep data off plot edges\nplt.margins(0.02)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T20:05:27.401374Z","iopub.execute_input":"2021-10-10T20:05:27.402339Z","iopub.status.idle":"2021-10-10T20:05:27.667908Z","shell.execute_reply.started":"2021-10-10T20:05:27.402289Z","shell.execute_reply":"2021-10-10T20:05:27.667008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Quantitative Exploratory Data Analysis\n\n> We often would like to summarize data even more succinctly, say in one or two numbers. \n\nThese numerical summaries are **not by any stretch** a substitute for the graphical methods , they do take up less real state. ","metadata":{}},{"cell_type":"code","source":"z = df_2[df_2.state == 'PA']\nnp.mean(z.dem_share) # this computes the mean for a column.","metadata":{"execution":{"iopub.status.busy":"2021-10-10T20:05:27.669137Z","iopub.execute_input":"2021-10-10T20:05:27.66941Z","iopub.status.idle":"2021-10-10T20:05:27.677311Z","shell.execute_reply.started":"2021-10-10T20:05:27.669359Z","shell.execute_reply":"2021-10-10T20:05:27.676415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We might like a summary statistic that is immune to extreme data.\n**The median**: provides exactly that, the middle value of a dataset.","metadata":{}},{"cell_type":"code","source":"np.median(z.dem_share)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T20:05:27.680686Z","iopub.execute_input":"2021-10-10T20:05:27.681132Z","iopub.status.idle":"2021-10-10T20:05:27.694915Z","shell.execute_reply.started":"2021-10-10T20:05:27.68109Z","shell.execute_reply":"2021-10-10T20:05:27.693895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.1 Percentiles, outliers, and boxplots\n*The median is the special name for the (50th percentile), that is 50% of the data are less than the median.*\n\nSimilarly, the(25th percentile) is the value of the data point that is greater than 25% of the sorted data, and so on for any other percentile we want.\n\n**Percentiles** are useful summary statistics, and can be computed --> np.percentile()\n*It takes a column data as a first argument and a list of the percentiles we want as a second argument (Percentiles NOT Fractions)*\n\n**Now, we've three summary statistics.**The whole point of summary statistics was to keep things concise, but we're starting to get a lot of numbers here.\n\n**Dealing with this issue is where \"Quantitative\" EDA meets \"Graphical\" EDA**\n\n**Box Plots** --> were invented to display some of the statistical or salient features of the dataset based on percentiles.\n\n==> The center of the box is the median (50th percentile), the edges of the box are the 25th & 75th percentiles. The total height of the box contains the middle 50% of the data, and is called (the interquartile) range, the whiskers extend a distance of (1.5) * IQR, or the extend of the data.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/max/8000/1*0MPDTLn8KoLApoFvI0P2vQ.png\" alt=\"Drawing\" style=\"width: 900px;\" />","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T20:05:27.69626Z","iopub.execute_input":"2021-10-10T20:05:27.69667Z","iopub.status.idle":"2021-10-10T20:05:27.712772Z","shell.execute_reply.started":"2021-10-10T20:05:27.696641Z","shell.execute_reply":"2021-10-10T20:05:27.711878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"percentiles = np.array([2.5, 25, 50, 75, 97.5])\n                        \nvoters_percentiles = np.percentile(df.dem_share, percentiles)\nprint(voters_percentiles)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T20:07:14.882074Z","iopub.execute_input":"2021-10-10T20:07:14.882343Z","iopub.status.idle":"2021-10-10T20:07:14.889773Z","shell.execute_reply.started":"2021-10-10T20:07:14.882314Z","shell.execute_reply":"2021-10-10T20:07:14.888858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = sns.boxplot(x='east_west', y='dem_share', data=df)\n\n_ = plt.xlabel(\"region\")\n_ = plt.ylabel(\"Percent of vote for Obama\")","metadata":{"execution":{"iopub.status.busy":"2021-10-10T20:15:39.406896Z","iopub.execute_input":"2021-10-10T20:15:39.407576Z","iopub.status.idle":"2021-10-10T20:15:39.567793Z","shell.execute_reply.started":"2021-10-10T20:15:39.407528Z","shell.execute_reply":"2021-10-10T20:15:39.566892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.2 Variance and standard deviation \n\n**What about the variability, or the spread of the data ?**\n\n**Variance** --> the mean (Average) squared distance of the data from their mean. To quantify this spread, informally, a measure of the spread of the data.\n\n$$ For each data point, we square the distance from the mean, and then take the average of all of these values.   \n\nthis is calculated with --> np.var() \n \n\n<b> Now, because the calculations of the varience invloves squared quantities, it does not have the same units of what we've measured. Therefore, we are interested in the square root of the varience.\n </b>\n \n <b>Standard Deviation</b> --> Is the square root of the varience. \n \n **A low standard deviation indicates that the values tend to be close to the mean of the set, while a high standard deviation indicates that the values are spread out over a wider range.**\n \n this is calculated with --> np.std() \n \n *Is a reasonable metric for the typical spread of the data.*","metadata":{}},{"cell_type":"code","source":"# Array of differences to mean: differences\ndifferences = df.dem_share - np.mean(df.dem_share)\n\n\n# Square the differences: diff_sq\ndiff_sq = differences ** 2\n\n# Compute the mean square difference: variance_explicit\nvariance_explicit = np.mean(diff_sq)\n\n# Compute the variance using NumPy: variance_np\n\nvariance_np = np.var(df.dem_share)\n# Print the results\nprint(variance_explicit , variance_np)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T20:38:53.028754Z","iopub.execute_input":"2021-10-10T20:38:53.029621Z","iopub.status.idle":"2021-10-10T20:38:53.038068Z","shell.execute_reply.started":"2021-10-10T20:38:53.029579Z","shell.execute_reply":"2021-10-10T20:38:53.037448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute the variance: variance\nvariance = np.var(df.dem_share)\n\n# Print the square root of the variance\nprint(np.sqrt(variance))\n\n# Print the standard deviation\nprint(np.std(df.dem_share))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T20:39:49.276109Z","iopub.execute_input":"2021-10-10T20:39:49.276377Z","iopub.status.idle":"2021-10-10T20:39:49.282635Z","shell.execute_reply.started":"2021-10-10T20:39:49.276349Z","shell.execute_reply":"2021-10-10T20:39:49.281753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.3 Covarience and the pearson correlation coefficient\n\n*We would like to have a summary statistic to go along with the information we have just gleaned from the scatter plot below.*\n\n**Covarience** --> A measure of how two quantities vary together.  \n\n*np.cov()*\n\nto understand where it comes from, let's annotate the scatter plot with the means of the two quantities we're interested in.\n\nThe covarience is the mean of the product of these differences. \n\n**If (x) & (y) both tend to be above, or both below their respective means together.**\n\nas they are in this dataset, then the covariance is positive. \n\n*This means that they are positively correlated: when (x) is high so is (y)*\n\nConversely, if (x) is high while (y) is low, the coveriance is negative.\n\n**If we want to have a more generally applicable measure of how two variables depend on each other, we want it to be dimensionless, that is to not have any units.**\n\n**Pearson correlation coefficient** --> It is a comparison of the variability in the data due to covarience to the variablility inherent to each variable independently (their standard deviation ) * It ranges from (-1) to (1)*\n\nnp.corrcoef()\n\nAnother definition for PCC -->  is the covariance of the two variables divided by the product of their standard deviations.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://usersolutions.com/wp-content/uploads/2014/03/Covariance.png\" alt=\"Drawing\" style=\"width: 350px;\" />","metadata":{}},{"cell_type":"markdown","source":"![image.png](attachment:80af24e3-34da-43a4-b4ec-fe1515816123.png)","metadata":{},"attachments":{"80af24e3-34da-43a4-b4ec-fe1515816123.png":{"image/png":"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"}}},{"cell_type":"code","source":"_ = sns.scatterplot(x='total_votes', y='dem_share', data=df_2)\n_ = plt.xlabel(\"Percent of vote for Obama\")\n_ = plt.ylabel(\"Total votes (thousands)\")","metadata":{"execution":{"iopub.status.busy":"2021-10-10T21:18:15.027621Z","iopub.execute_input":"2021-10-10T21:18:15.028441Z","iopub.status.idle":"2021-10-10T21:18:15.308395Z","shell.execute_reply.started":"2021-10-10T21:18:15.028368Z","shell.execute_reply":"2021-10-10T21:18:15.307338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pearson_r(x, y):\n    \"\"\"Compute Pearson correlation coefficient between two arrays.\"\"\"\n    # Compute correlation matrix: corr_mat\n    corr_mat = np.corrcoef(x,y)\n\n    # Return entry [0,1]\n    return corr_mat[0,1]","metadata":{"execution":{"iopub.status.busy":"2021-10-10T21:44:02.675558Z","iopub.execute_input":"2021-10-10T21:44:02.677787Z","iopub.status.idle":"2021-10-10T21:44:02.68722Z","shell.execute_reply.started":"2021-10-10T21:44:02.677701Z","shell.execute_reply":"2021-10-10T21:44:02.686137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_2.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T21:44:53.535378Z","iopub.execute_input":"2021-10-10T21:44:53.536407Z","iopub.status.idle":"2021-10-10T21:44:53.562509Z","shell.execute_reply.started":"2021-10-10T21:44:53.536343Z","shell.execute_reply":"2021-10-10T21:44:53.561421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = sns.scatterplot(x='dem_votes', y='rep_votes', data=df_2)\n_ = plt.xlabel(\"Percent of vote for Obama\")\n_ = plt.ylabel(\"Total votes (thousands)\")","metadata":{"execution":{"iopub.status.busy":"2021-10-10T21:46:08.82615Z","iopub.execute_input":"2021-10-10T21:46:08.826513Z","iopub.status.idle":"2021-10-10T21:46:09.139012Z","shell.execute_reply.started":"2021-10-10T21:46:08.826477Z","shell.execute_reply":"2021-10-10T21:46:09.138083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute Pearson correlation coefficient \nr = pearson_r(df_2.dem_votes , df_2.rep_votes)\n\n# Print the result\nprint(r)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T21:46:09.329106Z","iopub.execute_input":"2021-10-10T21:46:09.329407Z","iopub.status.idle":"2021-10-10T21:46:09.335906Z","shell.execute_reply.started":"2021-10-10T21:46:09.329363Z","shell.execute_reply":"2021-10-10T21:46:09.334973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9. Probablistic logic and statistical inference.\n*Probablistic reasoning allows us to describe uncertainty.*\n\nfor example, though you can't tell me exactly what the mean of the next 50 petal lengths you measure will be, you could say \"That is more probably to be close to what you got in the first 50 measurments that it is to be much greater.\"\n\n<b>That is what probablistic thinking is all about --> given a set of data, you describe probablistically what you might expect if those data were acquired again and again <b/>\n    \n<b>This is the heart of statistical inference</b>\n\n<b>*It is the process by which we go from measured data to probablistic conclusion about what we might expect if we collected the same data again.* </b>\n    \n \"Your Data Speak In The Language Of Probablility\"","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://media.giphy.com/media/TWFrH2pBDUCnzwKsQz/giphy.gif\" alt=\"Drawing\" style=\"width: 350px;\" />","metadata":{}},{"cell_type":"markdown","source":"## 9.1 Discrete variables (Random number generator & hacker statistics)\n\nIn practice, we're going to think probablistically using hacker stats.\n\n**Hacker Statistics --> Uses simulated repeated measurments to compute probabilities**\n\n*np.random()* module, a suite of functions based on random number generation.\n\n*np.random.random()* --> Draw a number between 0 and 1. \n\n**Bernolli trail --> *An experiment that has two options, 'Success'(True) and 'Fail'(False)***\n\n*np.random.seed()* --> integer fed into random number generating algorithm. Manually seed random number generator if you need reproducibility.\n\n**Hacker stats probabilities --> Determine how to simulate data, simulate it many many times. Probability is approximately fraction of trails with the outcome of interest.** ","metadata":{}},{"cell_type":"code","source":"## E.x : Coin flips --> \nnp.random.random(size=4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}